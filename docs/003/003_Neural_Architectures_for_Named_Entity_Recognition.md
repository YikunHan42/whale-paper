# Neural Architectures for Named Entity Recognition

# 用于命名实体识别的神经架构

## 背景介绍

命名实体识别（NER）是自然语言处理中的一个重要任务，最先进的系统通常依赖于手工创建的特征和特定领域的知识。在神经网络出现之前，几乎所有NER半监督或者非监督的方法，都要依靠手工的单词特征或者外部的监督库(如gazetteer) 达到最好的识别效果。

手工的单词特征可以方便提炼出类似前缀，后缀，词根

而外部的监督库(如gazetteer)，把一些同种类的实体聚合在一起做成个库，可以帮助识别同一个意思的实体，如:

auntie其实和aunt一个意思：姨妈

Mikey其实是Mike的昵称，都是人名

然而，这些方法泛化能力弱。本文介绍了两种新的神经网络架构，旨在解决这些问题。

## 模型架构、方法

### 编码

![Image text](https://github.com/datawhalechina/whale-paper/blob/master/docs/003/images/ch1.png)

**"Mars"（火星）一词的字符嵌入值被赋予一个双向 LSTM。我们将它们的最后输出与查找表中的嵌入式连接起来，从而获得该单词的表示。**

对一个单词的每个字符进行双向LSTM编码，再加入预训练的单词编码。使用dropout把他们结合起来。

### LSTM-CRF模型

![Image text](https://github.com/datawhalechina/whale-paper/blob/master/docs/003/images/ch2.png)

**网络的主要架构。li 表示单词 i 及其左语境，ri 表示单词 i 及其右语境。将这两个向量连接起来，就得到了单词 i 在其上下文中的表示，即 ci。**

Bi-LSTM:上下文关系

CRF: 词性之间的关系

e.g.:吃(动词)后大概率接“食物”等实体，而不是“地点”

本文提出了两种神经网络架构：LSTM-CRF和S-LSTM。LSTM-CRF模型使用双向LSTM和条件随机场层，而S-LSTM模型受到移位还原解析器的启发，使用基于转换的方法构建和标记词段。这两种模型都使用了基于字符的单词表示和分布表示来捕捉正字法和分布敏感性。

### 训练方法

首先，使用反向传播算法训练网络，每个训练实例一次更新一个参数。采用随机梯度下降算法（SGD），学习率为0.01，梯度剪切为5.0。然而，为了提高SGD的性能，研究人员提出了其他方法，如Adadelta和Adam。尽管这些方法的收敛速度更快，但它们的性能不如带有梯度削波的SGD。

对于LSTM-CRF模型，使用单层的前向和后向LSTM，维度设置为100。调整这一维度对模型性能没有明显影响。丢弃率被设定为0.5，较高的丢弃率会对结果产生负面影响，而较小的丢弃率会延长训练时间。

堆栈-LSTM模型使用两个层，每个层的维度为100。合成函数中使用的动作嵌入各有16个维度，输出嵌入的维度为20。研究人员尝试了不同的辍学率，并使用每种语言的最佳辍学率报告得分。

最后，堆栈-LSTM模型是一个贪婪模型，它应用局部最优动作，直到整个句子处理完毕。这可以通过束搜索或探索训练来实现。

## 实验分析

### 数据集

使用了CoNLL-2002 和 CoNLL-2003 数据集（Tjong Kim Sang，2002 Tjong Kim Sang 和 De Meulder，2003）。包含英语、西班牙语、德语和荷兰语。

### 对比

1. 在英语命名实体识别（NER）任务中，Luo等人（2015年）的模型得分最高。他们使用了大量人工设计的特征和外部知识库，如拼写特征、WordNet聚类、Brown聚类、POS标记、chunks标记以及词干和外部知识库（如Freebase和Wikipedia）。
2. 论文的LSTM-CRF模型在不使用地名录或任何外部标注资源的情况下，优于其他所有系统，包括使用外部标注数据的系统。
3. 除了Chiu和Nichols（2015年）提出的模型外，；论文的Stack-LSTM模型也优于之前所有不包含外部特征的模型。
4. 在德语、荷兰语和西班牙语的NER任务中，LSTM-CRF模型的表现明显优于之前的所有方法，包括使用外部标注数据的方法。
5. 唯一的例外是荷兰语，Gillick等人（2015年）的模型通过利用其他NER数据集的信息，可以取得更好的成绩。

综上所述，论文的LSTM-CRF模型在多个语言的NER任务中表现出色，并且在不使用外部标注数据的情况下仍然优于其他模型。Stack-LSTM模型在某些情况下也取得了最先进或接近最先进的结果。

评价指标包括准确率和召回率。

## 总结

本文介绍了两种基于神经网络的NER模型，这些模型不依赖于特定语言的资源或特征。作者使用了有监督训练数据和未标注的语料库进行训练，并通过反向传播算法进行参数更新。实验结果表明，这些模型在各种语言的NER任务中取得了最先进的性能。这些模型的关键特点是使用CRF架构或基于转换的算法来构建和标注输入块，并使用基于字符的单词表示和分布表示来捕捉正字法和分布敏感性。

## 参考文献

论文中Stack LSTM相关内容请参考这篇文章：Dyer, C, Ballesteros, M, Ling, W,, Matthews, A., Smith, N.A, 2015. Transition-Based Dependency Parsingwith Stack Long Short-Term Memory. arXiv:1505.08075[cs].

LSTM-CRF 和 Stack-LSTM NER 系统的代码可从以下网址获取 https://github.com/glample/tagger and https://github.com/clab/stack-lstm-ner